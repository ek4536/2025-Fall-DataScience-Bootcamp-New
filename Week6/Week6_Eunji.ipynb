{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "**Name:** Eunji Kim\n",
    "**Email:** ek4536@nyu.edu\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff309fe",
   "metadata": {},
   "source": [
    "1.  **Strategy for Generalizing to New Speakers (Group-based Split)**\n",
    "\n",
    "    To ensure the model generalizes to **\"new speakers,\"** you must split the data **by speaker (person)**, not by individual recordings.\n",
    "    * **Strategy:** First, split the list of 100 people into training, validation, and test groups (e.g., 80 people / 10 people / 10 people).\n",
    "    * **Data Allocation:**\n",
    "        * **Training Set:** All recordings from the 80 people in the train group (80 people * 5 days of data).\n",
    "        * **Validation Set:** All recordings from the 10 people in the validation group.\n",
    "        * **Test Set:** All recordings from the remaining 10 people in the test group.\n",
    "    * **Reason:** This ensures that the model is tested on voices it has **never heard before** during training. If you split randomly by file, the same speaker's voice would be in both the train and test sets, leading to **data leakage** and an overly optimistic (and incorrect) performance metric.\n",
    "\n",
    "2.  **Strategy for Adding Kilian's Data (Transfer Learning / Fine-tuning)**\n",
    "\n",
    "    The goal is to specialize the model for Kilian while maintaining its general performance. We can use **Transfer Learning**, specifically **Fine-tuning**.\n",
    "    * **Step 1 (Pre-training):** First, train a \"general voice model\" on the large dataset from the 100 speakers (from part 1).\n",
    "    * **Step 2 (Kilian's Data Split):** Split Kilian's 10,000 recordings into a fine-tuning set (e.g., 9,000) and a Kilian-specific validation set (e.g., 1,000).\n",
    "    * **Step 3 (Fine-tuning):** Take the pre-trained \"general model\" from Step 1 and continue training it **only** on Kilian's 9,000 fine-tuning samples. It is crucial to use a **very small learning rate** to avoid \"forgetting\" the general knowledge.\n",
    "    * **Step 4 (Evaluation):**\n",
    "        * **Kilian's Performance:** Evaluate the model on Kilian's 1,000 validation samples.\n",
    "        * **General Performance:** Re-evaluate the model on the original \"new speaker\" test set (from part 1) to ensure its general performance has not been significantly degraded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before Scaling ---\n",
      "Test Point: [500   1]\n",
      "Prediction: 1 (1 = Positive, -1 = Negative)\n",
      "\n",
      "--- After Scaling ---\n",
      "Original X:\n",
      "[[100   2]\n",
      " [100   4]\n",
      " [500   4]\n",
      " [300   1]\n",
      " [300   2]]\n",
      "Scaled X:\n",
      "[[0.         0.33333333]\n",
      " [0.         1.        ]\n",
      " [1.         1.        ]\n",
      " [0.5        0.        ]\n",
      " [0.5        0.33333333]]\n",
      "Original Test Point: [500   1], Scaled Test Point: [1. 0.]\n",
      "Prediction: -1 (1 = Positive, -1 = Negative)\n"
     ]
    }
   ],
   "source": [
    "# (a) Visualize 1-NN decision boundary for given points\n",
    "pos = np.array([[3,7],[3,3],[6,3]])\n",
    "neg = np.array([[7,4],[8,7],[5,9]])\n",
    "\n",
    "# Plotting decision regions for 1-NN\n",
    "X_train = np.vstack([pos, neg])\n",
    "y_train = np.array([1]*len(pos) + [0]*len(neg))\n",
    "knn = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
    "\n",
    "x_min, x_max = 0, 10\n",
    "y_min, y_max = 0, 10\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.2, cmap='coolwarm')\n",
    "plt.scatter(pos[:,0], pos[:,1], color='blue', label='Positive')\n",
    "plt.scatter(neg[:,0], neg[:,1], color='red', label='Negative')\n",
    "plt.title(\"1-NN Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n",
    "\n",
    "# (b) Classification of (500,1) before/after scaling\n",
    "print(\"\\n--- (b) Effect of Feature Scaling ---\")\n",
    "\n",
    "# Define dataset\n",
    "X = np.array([[1,1],[500,1],[1,500],[500,500]])\n",
    "y = np.array([1,0,0,1])  # arbitrary labels\n",
    "test_point = np.array([[500,1]])\n",
    "\n",
    "# 1-NN before scaling\n",
    "knn_raw = KNeighborsClassifier(n_neighbors=1).fit(X, y)\n",
    "pred_raw = knn_raw.predict(test_point)[0]\n",
    "print(f\"Prediction before scaling: {pred_raw}\")\n",
    "\n",
    "# 1-NN after min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "test_scaled = scaler.transform(test_point)\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=1).fit(X_scaled, y)\n",
    "pred_scaled = knn_scaled.predict(test_scaled)[0]\n",
    "print(f\"Prediction after scaling: {pred_scaled}\")\n",
    "\n",
    "print(\"\"\"\n",
    "Explanation:\n",
    "Without scaling, the first feature (500) dominates distance computation.\n",
    "After min–max normalization to [0,1], both features contribute equally.\n",
    "So the nearest neighbor — and thus classification — may change.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "--- (c) Handling missing features in K-NN ---\n",
    "Use one of:\n",
    "1. Impute missing values (mean/median per feature).\n",
    "2. Compute distances only on non-missing dimensions (masked distance).\n",
    "3. Use KNNImputer in scikit-learn.\n",
    "\n",
    "--- (d) Why KNN can work for images ---\n",
    "Images are flattened into vectors or represented by feature embeddings (e.g. from CNNs).\n",
    "Distances then capture visual similarity, enabling KNN to classify image classes.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "(a) Evaluating 'sign(w·x)' on training vs. test set:\n",
    "- On training data: shows if Perceptron learned to separate correctly.\n",
    "- On test data: measures generalization to unseen examples.\n",
    "We rarely recompute training error repeatedly because Perceptron\n",
    "updates only on mistakes — it keeps improving until convergence.\n",
    "\"\"\")\n",
    "\n",
    "# (b) Two-point dataset\n",
    "pos = np.array([[10, -2]])\n",
    "neg = np.array([[12, 2]])\n",
    "X = np.vstack([pos, neg])\n",
    "y = np.array([1, -1])\n",
    "w = np.zeros(2)\n",
    "lr = 1\n",
    "\n",
    "weights = [w.copy()]\n",
    "converged = False\n",
    "epoch = 0\n",
    "while not converged and epoch < 20:\n",
    "    converged = True\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi * np.dot(w, xi) <= 0:\n",
    "            w = w + lr * yi * xi\n",
    "            weights.append(w.copy())\n",
    "            converged = False\n",
    "    epoch += 1\n",
    "\n",
    "print(\"Weight sequence:\")\n",
    "for i, wv in enumerate(weights):\n",
    "    print(f\"Update {i}: w = {wv}\")\n",
    "print(f\"Total updates: {len(weights)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([[10, -2], [12, 2]])\n",
    "y = np.array([1, -1])\n",
    "w = np.zeros(2)\n",
    "lr = 1\n",
    "\n",
    "weights = [w.copy()]\n",
    "for epoch in range(6):  # show first few updates\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi * np.dot(w, xi) <= 0:\n",
    "            w = w + lr * yi * xi\n",
    "            weights.append(w.copy())\n",
    "\n",
    "print(\"Weight updates:\")\n",
    "for i, wv in enumerate(weights):\n",
    "    print(f\"w{i} = {wv}\")\n",
    "\n",
    "print(\"\\nConclusion: The dataset is not linearly separable, \"\n",
    "      \"so the Perceptron never converges—it keeps oscillating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Given perceptron log:\n",
    "learning rate = 1\n",
    "initial weights = (0,0)\n",
    "Data:\n",
    "x1 = (3,3), y1 = +1\n",
    "x2 = (1,1), y2 = -1\n",
    "x3 = (2,2), y3 = +1\n",
    "\n",
    "Assume updates occurred when misclassified.\n",
    "\n",
    "Let's reconstruct programmatically:\n",
    "\"\"\")\n",
    "\n",
    "X = np.array([[3,3],[1,1],[2,2]])\n",
    "y = np.array([1,-1,1])\n",
    "w = np.zeros(2)\n",
    "lr = 1\n",
    "\n",
    "for xi, yi in zip(X, y):\n",
    "    if yi * np.dot(w, xi) <= 0:\n",
    "        w = w + lr * yi * xi\n",
    "\n",
    "print(f\"Final weight vector: {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small 2D dataset\n",
    "X = np.array([[2,3],[4,2],[3,6],[5,5]])\n",
    "y = np.array([1,-1,1,-1])\n",
    "w = np.zeros(2)\n",
    "lr = 1\n",
    "weights = [w.copy()]\n",
    "\n",
    "for epoch in range(10):\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi * np.dot(w, xi) <= 0:\n",
    "            w = w + lr * yi * xi\n",
    "            weights.append(w.copy())\n",
    "\n",
    "# Plot data\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[y==1][:,0], X[y==1][:,1], color='blue', label='Positive')\n",
    "plt.scatter(X[y==-1][:,0], X[y==-1][:,1], color='red', label='Negative')\n",
    "\n",
    "# Plot decision boundaries for each update\n",
    "x_vals = np.linspace(0,6,100)\n",
    "for i, wv in enumerate(weights):\n",
    "    if wv[1] != 0:\n",
    "        y_vals = -(wv[0]/wv[1])*x_vals\n",
    "        plt.plot(x_vals, y_vals, '--', alpha=0.3)\n",
    "\n",
    "plt.title(\"Perceptron Convergence (each dashed line = update)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "Each dashed line shows how the decision boundary shifts after an update.\n",
    "Eventually, the updates stop (convergence) once all points are correctly classified.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
